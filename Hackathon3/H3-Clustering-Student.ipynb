{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5  color=#003366> <b>[LEPL1109] - STATISTICS AND DATA SCIENCES</b> <br><br> \n",
    "<b>Hackathon 03 - Clustering: Bias in sensitive datasets</b> </font> <br><br><br>\n",
    "\n",
    "<font size=4  color=#003366>\n",
    "Prof. D. Hainaut<br>\n",
    "Prof. L. Jacques<br>\n",
    "\n",
    "<br><br>\n",
    "Adrien Banse (adrien.banse@uclouvain.be)<br>\n",
    "Jana Jovcheva (jana.jovcheva@uclouvain.be)<br>\n",
    "François Lessage (francois.lessage@uclouvain.be)<br>\n",
    "Sofiane Tanji (sofiane.tanji@uclouvain.be)<br>\n",
    "<div style=\"text-align: right\"> Version 2024-2025</div>\n",
    "<br><br>\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>[IMPORTANT] Read all the documentation</b>  <br>\n",
    "    Make sure that you read the whole notebook, <b>and</b> the <code>README.md</code> file in the folder.\n",
    "</div>\n",
    "<br><br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Guidelines and Deliverables**\n",
    "\n",
    "*   This hackathon is due on the **22 December 2024 at 22h00**\n",
    "*   Copying code or answers from other groups (or from the internet) is strictly forbidden. <b>Each source of inspiration (stack overflow, git, other groups, ChatGPT...) must be clearly indicated!</b>\n",
    "*  This notebook (with the \"ipynb\" extension) file, the report (PDF format) and all other files that are necessary to run your code must be delivered on <b>Moodle</b>.\n",
    "* Only the PDF report and the python source file will be graded, both on their content and the quality of the text / figures.\n",
    "  * 5/10 for the code.\n",
    "  * 4/10 for the Latex report.\n",
    "  * 1/10 for the visualization. <br><br>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[DELIVERABLE] Summary</b>  <br>\n",
    "After the reading of this document (and playing with the code!), we expect you to provide us with:\n",
    "<ol>\n",
    "   <li> a PDF file (written in LaTeX) that answers all the questions below. The report should contain high quality figures with named axes (we recommend saving plots with the <samp>.pdf</samp> extension);\n",
    "   <li> this Jupyter Notebook (it will be read, checked for plagiarism and evaluated);\n",
    "   <li> and all other files we would need to run your code.\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "As mentioned above, plagiarism is forbidden. However, we cannot forbid you to use artificial intelligence BUT we remind you that the aim of this project is to learn on your own and with the help of the course material. Finally, we remind you that for the same question, artificial intelligence presents similar solutions, which could be perceived as a form of plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Context & Objective**\n",
    "\n",
    "## Context\n",
    "\n",
    "Predictive algorithms serve multiple functions in criminal justice. They forecast crime locations, identify potential violent offenders, predict court appearance compliance, and estimate recidivism risk. \n",
    "COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) stands as a prominent risk assessment tool. Since 1998, the COMPAS risk score has been used by many jurisdictions in the United States to assess risk of recidivism in pre-trial bail decisions.\n",
    "In the United States, a defendant may either be detained or released on bail *(sous caution)* prior to the trial in court depending on various factors. Judges may detain defendants or increase the bail amount based on the risk score provided by the COMPAS algorithm.\n",
    "\n",
    "\n",
    "In 2016, investigative journalists at ProPublica published [Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), highlighting significant biases in the COMPAS Algorithm. Specifically, they showed that the proportion of false positives for African-American defendants is significantly higher than for Caucasian defendants. In other words, more African-American were labeled high risk and ended up not relapsing into criminal behaviour than Caucasian defendants. A more thorough explanation of their data analysis procedure can be found in their companion article [How We Analyzed the COMPAS Recidivism Algorithm](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm).\n",
    "\n",
    "The COMPAS algorithm is proprietary software. What is known is that its decision is based on the answers to a questionnaire with 137 questions which the defendant must fill. There are questions related to crime (“How many prior juvenile felony offense arrests?”) along with seemingly mundane ones (“Do you live with friends?”; “Do you feel discouraged at times?”).\n",
    "\n",
    "In this hackathon, you are provided a dataset with a subset of answers to that questionnaire from more than 7000 defendants living in Broward County, Florida as well as whether they did relapse into criminal behaviour or not.\n",
    "\n",
    "## Objective(s)\n",
    "It has been shown in the article linked above that the COMPAS algorithm is biased. We take this for granted and we do not bother to show it again. The main objective of the hackathon for you is to understand that it is not only the COMPAS algorithm that is biased, but that **the data itself is biased**, in the sense that one can find structural patterns of discrimination embedded in the data. In other words, *learning from data coming from a biased world without precautions will necessarily lead to biased predictions*. Knowing which precautions one should take to avoid biased predictions is a whole subfield of machine learning called \"Fairness in AI\". It is out of the scope of this hackathon and out of the scope of LEPL1109. If you are curious about it however, a great resource is the following book [Fairness and machine learning Limitations and Opportunities](https://fairmlbook.org/).\n",
    "\n",
    "To see that the data itself is biased, you will implement your own recidivism prediction algorithms and measure their fairness.\n",
    "\n",
    "## Dataset description\n",
    "A large part of this hackathon will be devoted to handling, understanding and manipulating the dataset. The dataset provided represents 7214 defendants (data points) with 49 answers to the questionnaire (features) for each defendant. This is a lot of features and it should take you some time to understand them before going through Part 1. This data processing part may be tiresome but it is a necessary task in any serious data project.\n",
    "A description of the features is provided in the table below:\n",
    "\n",
    "| Feature               | Description                                                                                      |\n",
    "|-----------------------|--------------------------------------------------------------------------------------------------|\n",
    "| name                  | Full name of the defendant                                                                       |\n",
    "| first                 | First name of the defendant                                                                      |\n",
    "| last                  | Last name of the defendant                                                                       |\n",
    "| compas_screening_date | The date the defendant filled the questionnaire                                                  |\n",
    "| sex                   | Sex of the defendant (Female, Male)                                                              |\n",
    "| dob                   | Date of birth of the defendant (YYYY-MM-DD)                                                      |\n",
    "| age                   | Age of the defendant                                                                             |\n",
    "| age_cat               | Age category of the defendant (Less than 25, 25-45, Greater than 45)                             |\n",
    "| race                  | race attribute (African-American, Caucasian, Hispanic, Asian, Native American, Other)       |\n",
    "| juv_fel_count         | Number of juvenile felonies committed by the defendant                                           |\n",
    "| decile_score          | Decile of the COMPAS score                                                                       |\n",
    "| juv_misd_count        | Number of juvenile misdemeanors                                                                  |\n",
    "| juv_other_count       | Number of juvenile convictions that are not considered misdemeanors nor felonies                 |\n",
    "| priors_count          | Number of prior crimes committed                                                                 |\n",
    "| days_b_screening_arrest | Count of days between screening date and (original) arrest date                                |\n",
    "| c_jail_in             | Datetime at which the defendant entered jail (YYYY-MM-DD, hh:mm:ss)                              |\n",
    "| c_jail_out            | Datetime at which the defendant left jail (YYYY-MM-DD, hh:mm:ss)                                 |\n",
    "| c_case_number         | Case number for the current charge                                                               |\n",
    "| c_offense_date        | Date the offense was committed (YYYY-MM-DD)                                                      |\n",
    "| c_arrest_date         | Date the offense was arrested (YYYY-MM-DD)                                                       |\n",
    "| c_days_from_compas    | Days from COMPAS screening date to current arrest date                                           |\n",
    "| c_charge_degree       | Current charge degree (felony or misdemeanor) at the time of filling the questionnaire (\"F\", \"M\")|\n",
    "| c_charge_desc         | Description of the current charge                                                                |\n",
    "| is_recid              | Binary variable indicating whether the defendant is rearrested at any time (0, 1)                |\n",
    "| r_case_number         | Case number for a recidivism charge                                                              |\n",
    "| r_charge_degree       | Recidivism charge degree (felony or misdemeanor) for an offense subsequent to filling the questionnaire |\n",
    "| r_days_from_arrest    | Days from Arrest to Recidivism Event                                                             |\n",
    "| r_offense_date        | Date the recidivism offense was committed (YYYY-MM-DD)                                           |\n",
    "| r_charge_desc         | Description of the recidivism charge                                                             |\n",
    "| r_jail_in             | Datetime at which the defendant entered jail for a recidivism charge (YYYY-MM-DD, hh:mm:ss)      |\n",
    "| r_jail_out            | Datetime at which the defendant left jail for a recidivism charge (YYYY-MM-DD, hh:mm:ss)         |\n",
    "| violent_recid         | Number of violent recidivism events                                                              |\n",
    "| is_violent_recid      | Binary variable indicating whether the defendant committed a violent recidivism (0, 1)           |\n",
    "| vr_case_number        | Case number for a violent recidivism charge                                                      |\n",
    "| vr_charge_degree      | Violent recidivism charge degree (felony or misdemeanor)                                         |\n",
    "| vr_offense_date       | Date the violent recidivism offense was committed (YYYY-MM-DD)                                   |\n",
    "| vr_charge_desc        | Description of the violent recidivism charge                                                     |\n",
    "| type_of_assessment    | Type of COMPAS assessment performed                                                              |\n",
    "| decile_score.1        | *Same as decile_score*                                                                           |\n",
    "| score_text            | Recidivism risk of the defendant (Low, Medium, High)                                             |\n",
    "| screening_date        | Date on which the defendant was screened (YYYY-MM-DD)                                            |\n",
    "| v_type_of_assessment  | Type of violent risk assessment                                                                  |\n",
    "| v_decile_score        | Decile score for violent risk assessment                                                         |\n",
    "| v_score_text          | Violent recidivism risk of the defendant (Low, Medium, High)                                     |\n",
    "| v_screening_date      | Date of the violent risk assessment (YYYY-MM-DD)                                                 |\n",
    "| in_custody            | Date on which the defendant was placed in custody (YYYY-MM-DD)                                   |\n",
    "| out_custody           | Date on which the defendant left custody (YYYY-MM-DD)                                            |\n",
    "| priors_count.1        | *Same as priors_count*                                                                           |\n",
    "| two_year_recid        | Binary variable on whether the defendant has recidivated within two years (0, 1)                 |\n",
    "\n",
    "## **Notebook structure**\n",
    "\n",
    "### PART 1 - Data preprocessing\n",
    "   #### 1.1 - Importing the packages\n",
    "   #### 1.2 - Importing the dataset\n",
    "   #### 1.3 - Dataset curation\n",
    "   #### 1.4 - Feature engineering\n",
    "   #### 1.5 - Sensitive features\n",
    "   #### 1.6 - Scale the dataset\n",
    "\n",
    "### PART 2 - Data exploration\n",
    "   #### 2.1 - Feature visualization\n",
    "   #### 2.2 - Principal Component Analysis\n",
    "\n",
    "   \n",
    "### PART 3 - Clustering\n",
    "   #### 3.1 - K-Means\n",
    "   #### 3.2 - Results Analysis\n",
    "\n",
    "\n",
    "### PART 4 - Validation and fairness metrics\n",
    "   #### 4.1 - Silhouette score\n",
    "   #### 4.2 - Purity and entropy of a clustering\n",
    "   #### 4.3 - Precision and Recall per race group\n",
    "   #### 4.4 - Select the number of clusters\n",
    "\n",
    "\n",
    "### PART 5 - Visualization\n",
    "   #### 5.1 - Visualize your results\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***Remark***\n",
    "\n",
    "We filled this notebook with preliminary (trivial) code. This practice makes possible to run each cell, even the last ones, without throwing warnings once the dataset is imported. <b>Take advantage of this aspect to divide the work between all team members!</b> <br><br>\n",
    "Remember that many libraries exist in Python, so many functions have already been developed. Read the documentation and don't reinvent the wheel! You can import whatever you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART I - Preliminaries</b> </font> <br><br>\n",
    "\n",
    "<font size=5 color=#009999> <b>1.1 - Importing the packages</b> <br>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1.1 : IMPORTING ALL THE NECESSARY PACKAGES\n",
    "\n",
    "@pre:  /\n",
    "@post: The necessary packages should be loaded.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all the necessary packages here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>1.2 - Importing the dataset</b> <br>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1.2 : IMPORTING THE DATASET\n",
    "\n",
    "@pre:  /\n",
    "@post: The object `df` should contain a Pandas DataFrame corresponding to the file `compas-dataset.csv`.\n",
    "\"\"\"\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>1.3 - Dataset curation</b> <br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this hackathon, your goal is to **determine the risk of recidivism of an individual**. Therefore, you should be able to determine which features are useful for your application and remove the unnecessary ones. We provide a list of features to keep and ask you to add features to that list. This step may take more time than the others. It is important to carefully analyze each feature and its relevance for our goal.\n",
    "\n",
    "In this data cleaning task, you must remove redundant features, features that are not quantifiable and features that you believe are not linked to risk of recidivism. Yous should neither limit yourself to the provided list which is too short nor add all numeric features.\n",
    "You should also avoid data leakage. Except for the \"two_year_recid\" feature, do not keep features which represent true recidivism. For similar reasons, do not keep features linked to the predictions made by the COMPAS algorithm. Using predictions made by a supervised algorithm (which is trained using both the features matrix and the target variable) is effectively leaking information from the target variable.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.1] Removing unnecessary features </b>  <br>\n",
    "Can you already, a priori, detect that some features are useless?\n",
    "<ol>\n",
    "   <li> if yes, list those (useless) features and explain your choice;\n",
    "   <li> if not, then explain why it is better to wait.\n",
    "</ol>\n",
    "    Generally speaking, is it a good idea to remove a feature based on <i>a priori</i> knowledge, or doesn't it alter the final outcome?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1.3.1 : CURATION OF THE DATASET\n",
    "\n",
    "@pre:  A pandas.DataFrame `df` containing the dataset\n",
    "@post: A pandas.DataFrame `df` containing the dataset without outliers and with necessary features only\n",
    "\"\"\"\n",
    "\n",
    "# We also provide some code to remove outliers.\n",
    "df = df[\n",
    "    (\n",
    "        df[\"is_recid\"] != -1\n",
    "    )  # Data aggregator encoded is_recid = -1 whenever they couldn't find a COMPAS case.\n",
    "    & (\n",
    "        df[\"days_b_screening_arrest\"] <= 30\n",
    "    )  # More than 30 days between the day of arrest and the date when the questionnaire was filled => poor data quality\n",
    "    & (df[\"days_b_screening_arrest\"] >= -30)  # Same as above\n",
    "    & (\n",
    "        df[\"c_charge_degree\"] != \"O\"\n",
    "    )  # These are simple traffic offenses, they will never be charged with jail.\n",
    "]\n",
    "\n",
    "# For reasons made explicit later, keep at least these columns.\n",
    "columns_to_keep = [\n",
    "    \"sex\",\n",
    "    \"race\",\n",
    "    \"c_jail_in\",\n",
    "    \"c_jail_out\",\n",
    "    \"in_custody\",\n",
    "    \"out_custody\",\n",
    "    \"two_year_recid\",\n",
    "]\n",
    "\n",
    "df.info()\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data science, datasets are rarely tailored to specific applications. Instead, they typically originate from information collected over a certain period. It is the data scientist's responsibility to effectively utilize these datasets.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark 1.1]</b><br>\n",
    "In most real-world cases, the datasets you work with will contain artifacts like typos or missing data, which may need to be removed before using them in algorithms. In Pandas, missing data is represented as \"NaNs\" (Not a Number), though it applies to all missing objects, not just numbers.\n",
    "</div>\n",
    "\n",
    "Can you find a way to inspect your dataset and see if there are some missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1.3.2: INFORMATION ABOUT TYPES AND NANs\n",
    "@pre:  A pandas.DataFrame `df` containing the dataset.\n",
    "@post: Statistics and/or visualization on the presence of missing data in `df`.\n",
    "\"\"\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark 1.2] Each problem has its own solution</b> <br>\n",
    "There exist numerous ways to deal with missing information and we will discuss the two main approaches:\n",
    "<ol>\n",
    "   <li> you remove rows or columns that contain missing data;\n",
    "   <li> or you replace NaNs with another value. The latter can be a fixed value or computed to be, e.g., the mean of all non-NaNs values. The topic of replacing missing data, also called imputation of missing values, is very broad and complex, and there is no global solution that applies everywhere. Maybe you can find one that works well here?\n",
    "</ol>\n",
    "    \n",
    "You **should** read more about how to imput missing value [here](https://scikit-learn.org/stable/modules/impute.html). However, you will not be evaluated on how sophisticated your handling of NaNs is so, for this hackathon, do not spend an unreasonable amount of time on the next cell.\n",
    "</div> \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.2] Handling missing data </b>  <br>\n",
    "Given the dataset and the amount / type of missing information, what strategy do you propose to follow regarding missing data (NaNs)? <br> You can choose one or many of the following:\n",
    "<ol>\n",
    "   <li> drop features (column) with missing information; \n",
    "   <li> drop samples (row) with missing information;\n",
    "   <li> replace missing information with interpolation / extrapolation / simple substitution / ...\n",
    "</ol>\n",
    "Justify briefly your choice.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1.3.3: Handling missing values\n",
    "@pre:  A pandas.DataFrame `df` containing the dataset.\n",
    "@post: A pandas.DataFrame `df` containing the dataset with no missing values.\n",
    "\"\"\"\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>1.4 - Feature engineering</b> <br>\n",
    "</font>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark 1.3] New features extraction</b> <br>\n",
    "In the present case, some features in the dataset still need to be reworked in order to provide meaningful information. For example, working with datetimes might not be easy.\n",
    "</div>\n",
    "\n",
    "You may want to somehow incorporate the information about date and time into the dataset in a more **intelligent** manner than it was before. Again, there can be multiple solutions, and we will propose you a very simple one.\n",
    "\n",
    "For example, what is most important to predict the likelihood of recidivism: the exact dates at which each defendant entered and left jail/custody or the time spent in jail/custody?\n",
    "\n",
    "Note: 1) you should apply your solution to all the \"date/time\" features you kept. There should at least be the one hinted above. 2) Pandas has a to_datetime function that should prove useful !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1.4 : FEATURE ENGINEERING\n",
    "\n",
    "@pre:  A pandas.DataFrame `df` containing the dataset\n",
    "@post: A pandas.DataFrame `df` containing the previous dataset with the new features you created.\n",
    "\"\"\"\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.3] New features </b>  <br>\n",
    "What features have you added? If a particular manipulation has been applied, please explain.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>1.5 - Sensitive features</b> <br>\n",
    "</font>\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark 1.4] Sensitive features</b> <br>\n",
    "At this stage of the Hackathon, you still have two sensitive features, the sex attribute and the race attribute. As the end goal is to build a fair learning algorithm, you should not reasonably use these two features to determine if there is a risk of recidivism of the defendant.\n",
    "</div>\n",
    "\n",
    "To check if your learning techniques are unfair to particular subgroups of these features, you should **remove both features from the dataset while keeping them aside** to analyze the fairness of our learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1.5 : SENSITIVE FEATURES\n",
    "\n",
    "@pre:  A pandas.DataFrame `df` containing the dataset with sensitive features\n",
    "@post: A pandas.DataFrame `df` containing the dataset without sensitive features and separate numpy arrays for each sensitive feature \n",
    "as well as the true label array `y`.\n",
    "\"\"\"\n",
    "\n",
    "# Use the dictionaries below to encode numerically the sensitive features.\n",
    "race = ...\n",
    "race_map = {\n",
    "    \"African-American\": 0,\n",
    "    \"Caucasian\": 1,\n",
    "    \"Asian\": 2,\n",
    "    \"Other\": 3,\n",
    "    \"Hispanic\": 4,\n",
    "    \"Native American\": 5,\n",
    "}\n",
    "\n",
    "sex_map = {\"Male\": 0, \"Female\": 1}\n",
    "\n",
    "# As you approach the last step of the preprocessing, you should also store the target variable and remove it from the dataframe.\n",
    "# It is good practice to remove it just before the scaling so that its dimension corresponds to the number of data points in `df`.\n",
    "y = df[\"two_year_recid\"].astype(float).values\n",
    "df.drop(columns=[\"two_year_recid\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>1.6 - Scaling the dataset</b> <br>\n",
    "</font>\n",
    "\n",
    "***Standardizing*** is important when you work with data because it allows data to be compared with one another.\n",
    "\n",
    "$z$ is the standard score of a population $x$. It can be computed as follows:\n",
    "$$z = \\frac{x-\\mu}{\\sigma}$$\n",
    "with $\\mu$ the mean of the population and $\\sigma$ the standard deviation of the population.\n",
    "\n",
    "Please consult [Wikipedia](https://en.wikipedia.org/wiki/Standard_score) for further information about the standardization.\\\n",
    "Be careful to use the same formula as us, check in `scikit-learn` and check the already existing imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1.6 : SCALE THE DATASET\n",
    "\n",
    "@pre:  A pandas.DataFrame `df` containing the dataset\n",
    "@post: A pandas.DataFrame `df` containing the standardized dataset\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def scale_dataset(df):\n",
    "    # Modify here...\n",
    "    return df\n",
    "\n",
    "\n",
    "X = scale_dataset(df)\n",
    "X.info()\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART 2 - Data Exploration</b> </font> <br><br>\n",
    "\n",
    "<font size=5 color=#009999> <b>2.1 - Feature visualization</b> <br>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the dataset balanced in terms of sensitive groups ?\n",
    "It's good practice to check this to better understand the contents of our dataset.\n",
    "Indeed, if the training dataset is severely imbalanced, our learning algorithm may perform better for over-represented groups than for under-represented groups. Moreover, our goal is for the model to perform equally well across all groups.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 2.1] (Im)Balanced dataset ? </b>  <br>\n",
    "Is the dataset imbalanced ? What could be the consequences in terms of fairness i.e. in terms of the model performing equally well across all groups ?\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°2.1.1: (Im)Balanced dataset ?\n",
    "\n",
    "@pre:  A pandas.DataFrame `X` containing the dataset\n",
    "@post: A pie chart plot representing the repartition of race groups in the dataset.\n",
    "\"\"\"\n",
    "\n",
    "# As the race group was previously removed, we can temporarily add it back, using the following map.\n",
    "race_reverse_map = {v: k for k, v in race_map.items()}\n",
    "\n",
    "\n",
    "X = X.drop(columns=[\"race\"])  # Remove the race group again after doing the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "In order to check to the important features in our dataset, we can compute and plot (see e.g. `sns.heatmap`) the correlation matrix, as a tool to visually show all the correlation between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°2.1.2 : Correlation matrix\n",
    "\n",
    "@pre:  A pandas.DataFrame `df` containing the dataset\n",
    "@post: A visualization of the correlation matrix between features.\n",
    "\"\"\"\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>2.2 Principal Component Analysis</b> <br>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is often considered as the simplest and most fundamental technique used in dimensionality reduction. Remember that PCA is essentially the rotation of coordinate axes, chosen such that each successful axis captures or preserves as much variance as possible. If the algorithm returns a new system coordinates of the same dimension as the input, we can keep only the axis corresponding to the 3 largest singular values and project data on this coordinates system to perform the visualization.\n",
    "\n",
    "To vizualize the importance of features, we can extract the PCA loadings. These are indicators of the correlation between components and original features. The value of loadings is contained between -1 and 1. The more the value goes toward those boundaries, the more the feature influences the choice of component.We propose to perform a 2-dimensional PCA and then to add the loadings in vector form to the figure to obtain what is called a biplot.\n",
    "\n",
    "The biplot visualization function is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°2.2.1 : Principal Component Analysis (2D)\n",
    "\n",
    "@pre:  A pandas.DataFrame `X` containing the dataset and labels `y`\n",
    "@post: A PCA visualization in 2D where points are colored with respect to true labels `y`\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def biplot_visualization(X, y, columns=None):\n",
    "    \"\"\"\n",
    "    Plot a biplot graph: the scaled data after applying a 2D PCA with loadings in vector forms.\n",
    "\n",
    "    :param pca: PCA object\n",
    "    :param X: a n by m matrix (or DataFrame), containing the input prior to the PCA transformation\n",
    "    :param y: a vector of length n containing the target\n",
    "    :param columns: a list of length m contained the names of the columns\n",
    "        If not given, X.columns will be used\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=2)\n",
    "    X = pca.fit_transform(X)\n",
    "\n",
    "    columns = (\n",
    "        columns\n",
    "        if columns is not None\n",
    "        else X.columns\n",
    "        if isinstance(X, pd.DataFrame)\n",
    "        else [f\"Feature {i+1}\" for i in range(X.shape[1])]\n",
    "    )\n",
    "\n",
    "    # Normalize data for scaling\n",
    "    X_normalized = X / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "    df = pd.DataFrame(data=X_normalized, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "    # Prepare loadings (vector components)\n",
    "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "    loadings_df = pd.DataFrame(loadings, columns=[\"PC1\", \"PC2\"], index=columns)\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x=df[\"PC1\"], y=df[\"PC2\"], hue=y, palette=\"viridis\", s=70, alpha=0.7)\n",
    "\n",
    "    # Add vectors for loadings\n",
    "    for index, row in loadings_df.iterrows():\n",
    "        plt.arrow(\n",
    "            0,\n",
    "            0,\n",
    "            row.PC1,\n",
    "            row.PC2,\n",
    "            color=\"red\",\n",
    "            alpha=0.7,\n",
    "            head_width=0.02,\n",
    "            head_length=0.03,\n",
    "        )\n",
    "        plt.text(\n",
    "            row.PC1 * 1.1,\n",
    "            row.PC2 * 1.1,\n",
    "            index,\n",
    "            color=\"black\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    # Labels and limits\n",
    "    plt.title(\"Biplot Visualization\", fontsize=14)\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.axvline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend(title=\"Classes\", loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# biplot_visualization(X, y, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, you are asked to perform a 3 components PCA and plot it using Plotly.\n",
    "<div class=\"alert alert-danger\">\n",
    " Note: On certain versions of Firefox, the 3D scatter function of plotly may have some issues.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°2.2.2 : Principal Component Analysis (3D)\n",
    "\n",
    "@pre:  A pandas.DataFrame `X` containing the dataset and labels `y`\n",
    "@post: A PCA visualization in 3D where points are colored with respect to true labels `y`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 2.2] Principal Component Analysis </b>  <br>\n",
    "Do all features have the same importance? If no, which features are less important, and why? You can use all other graphs from the visualization part to justify your answer.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART 3 - Clustering</b> </font> <br><br>\n",
    "\n",
    "<font size=4 color=#009999> <b>ABCs of Clustering</b> <br>\n",
    "Clustering can be defined as the task of *grouping* objects from a set $S$ (here, each row/observation is an object) in such a way that objects assigned to the same group (called cluster) are more **similar** (or less **distant**) with respect to each other (in some sense) than to those assigned to the other groups. Usually, we would like to divide our objects into $K$ groups.\n",
    "\n",
    "As such, clustering reduces to finding, among all $K$-partitions possible of $S$, the partition $\\mathcal{P}$ that minimizes some error criterion $f(\\mathcal{P})$. Each object will be assigned a cluster, $C_i$, and each cluster will have its centroid $c_i$ the distance between **any object** in $C_i$ to centroid $c_i$ is **always smaller** that the distance to any other centroid. In other words, each object is assigned to the cluster whose centroid is the closest.\n",
    "\n",
    "\n",
    "A mathematical formulation of the problem could be the following, $$ \\boxed{\\min_{(C_1,\\dots,C_K) \\,\\in\\, \\mathcal{P}}\\,f(C_1,\\dots,C_K) = \\sum_{i = 1}^{K}\\,\\sum_{x \\in C_i}\\,\\Delta(x,c_i)}$$\n",
    "\n",
    "where $\\Delta(x,c_i)$ denotes the distance between object $x$ and centroid $c_i$.\n",
    "\n",
    "<br>\n",
    "<font size=5 color=#009999>\n",
    "EXAMPLE OF SEPARATING OBJECTS INTO 10 CLUSTERS\n",
    "</font> <br> <br>\n",
    "\n",
    "**First**, let us imagine the following 2D dataset.\n",
    "\n",
    "<img src=\"Imgs/10-partitions-data.svg\" width = \"250\">\n",
    "\n",
    "**Then**, a 10-partition is defined by the position of the centroids, one for each cluster. Below, you can observe four examples of (random) centroids localizations (stars).\n",
    "\n",
    "<img src=\"Imgs/10-partitions-chose-centroids.svg\" width = \"1000\">\n",
    "\n",
    "**Next**, the regions are colored based on their closest centroid. Here, we take the distance to be the Euclidean distance.\n",
    "\n",
    "<img src=\"Imgs/10-partitions-centroids.svg\" width = \"1000\">\n",
    "\n",
    "**Finally**, data points (objects) are colored based in the region they are in.\n",
    "\n",
    "<img src=\"Imgs/10-partitions-clusters.svg\" width = \"1000\">\n",
    "\n",
    "<font size=5 color=#009999> <b>3.1 - K-Means</b> <br>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3.1.1 : GROUND TRUTH\n",
    "\n",
    "@pre:  A pandas.DataFrame `X` containing the dataset and labels `y`\n",
    "@post: A 80/20 split of your dataset in train and test sets.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 3.1] Number of clusters </b>  <br>\n",
    "    Accounting for all features, what do you think is the ideal number of clusters? What will happen if too many or even too few clusters are chosen?\n",
    "</div>\n",
    "\n",
    "Now that your dataset is divided into a train and a test set, use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">KMeans</a> algorithm from `scikit-learn` to apply the clustering on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3.1.2 : K-Means\n",
    "\n",
    "@pre:  A split of your dataset: X_train, X_test, y_train, y_test\n",
    "@post: A split of your dataset in train and test sets.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_and_predict(model, X_train, X_test):\n",
    "    \"\"\"Trains the clustering model on the training data and predict the clusters for both training and test data.\n",
    "\n",
    "    Parameters:\n",
    "    model (sklearn or similar clustering model): The clustering algorithm that has a fit_predict method and a predict method.\n",
    "    X_train (array-like, shape (n_samples, n_features)): The training data to fit the model on.\n",
    "    X_test (array-like, shape (n_samples, n_features)): The test data to predict the clusters for.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two arrays:\n",
    "        - train_clusters (array): Cluster labels for the training data.\n",
    "        - test_clusters (array): Cluster labels for the test data.\n",
    "    \"\"\"\n",
    "    train_clusters = ...  # TODO\n",
    "    test_clusters = ...  # TODO\n",
    "    return train_clusters, test_clusters\n",
    "\n",
    "\n",
    "def compute_y_pred(model, X_train, X_test, y_train):\n",
    "    \"\"\"Compute the predicted labels for the test data based on the clustering model.\n",
    "\n",
    "    This function assigns a predicted label to each sample in the test set by:\n",
    "    1. Training the model on the training data using the previous function.\n",
    "    2. Assigning the majority class from the training labels to each cluster.\n",
    "    3. Using the cluster assignments from the test data to assign predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    model (sklearn or similar clustering model): The trained clustering model with an `n_clusters` attribute.\n",
    "    X_train (array-like, shape (n_samples, n_features)): The training data used to fit the model.\n",
    "    X_test (array-like, shape (n_samples, n_features)): The test data to predict labels for.\n",
    "    y_train (array-like, shape (n_samples,)): The true labels of the training data.\n",
    "\n",
    "    Returns:\n",
    "    np.array: An array of predicted labels for the test data based on the majority class in each cluster.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    train_clusters, test_clusters = train_and_predict(model, X_train, X_test)\n",
    "    df = pd.DataFrame({\"cluster\": train_clusters, \"target\": y_train})\n",
    "\n",
    "    for cluster in range(model.n_clusters):\n",
    "        majority_class = df[df[\"cluster\"] == cluster][\"target\"].mode()[0]\n",
    "        mapping[cluster] = majority_class\n",
    "\n",
    "    y_pred = ...  # TODO\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def compute_metrics(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Computes various evaluation metrics for the clustering model.\n",
    "\n",
    "    Parameters:\n",
    "    model (sklearn or similar clustering model): The trained clustering model with an `n_clusters` attribute.\n",
    "    X_train (array-like, shape (n_samples, n_features)): The training data used to fit the model.\n",
    "    X_test (array-like, shape (n_samples, n_features)): The test data to predict labels for.\n",
    "    y_train (array-like, shape (n_samples,)): The true labels of the training data.\n",
    "    y_test (array-like, shape (n_samples,)): The true labels of the test data.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the computed metrics:\n",
    "        - \"n_clusters\": The number of clusters in the model.\n",
    "        - \"Accuracy\": The accuracy of the model on the test data.\n",
    "        - \"F1-Score\": The F1-score of the model on the test data.\n",
    "        - \"Precision\": The precision of the model on the test data.\n",
    "        - \"Recall\": The recall of the model on the test data.\n",
    "        - \"Silhouette Score\": The silhouette score of the clustering on the test data.\n",
    "    \"\"\"\n",
    "    y_pred = compute_y_pred(model, X_train, X_test, y_train)\n",
    "    accuracy = ...  # TODO\n",
    "    f1 = ...  # TODO\n",
    "    precision, recall, _, _ = ...  # TODO\n",
    "    sil_score = ...  # TODO\n",
    "    return {\n",
    "        \"n_clusters\": ...,  # TODO\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Silhouette Score\": sil_score,\n",
    "    }\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=random_seed)\n",
    "# results = compute_metrics(kmeans, X_train, y_train, X_test, y_test)\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> <b>3.2 - Results Analysis</b> <br>\n",
    "</font>\n",
    "\n",
    "In this section, we adress the difficult task of evaluating the performance of the clustering algorithm.\n",
    "\n",
    "<font size=3 color=#009999> <b>3.2.1 - Quality of the clustering</b> <br>\n",
    "</font>\n",
    "The silhouette score is a measure of how close each point in one cluster is to points in the neighboring clusters. The [mean silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) is an average of the silhouette score for each point and provides a way to measure the quality of the clustering.\n",
    "\n",
    "The best value is 1 and the worst value is -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3.2.1 : Silhouette Score\n",
    "\n",
    "@pre:  A split of your dataset: X_train, X_test, y_train, y_test\n",
    "@post: A \"Mean Silhouette Score versus Number of Clusters\" plot\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>3.2.2 - Purity and entropy of a clustering</b> <br>\n",
    "</font>\n",
    "\n",
    "### Purity\n",
    "\n",
    "Purity measures how well a cluster contains points from a single class. A cluster with high purity mostly contains points from one class.\n",
    "\n",
    "**Example:** Imagine you are grouping fruits based on their shape, but you also have information about their color. If a group contains mostly red apples, that group has high purity. However, if you find a few green apples or pears in the group, the purity decreases. In this case, high purity means the majority of fruits share both shape and color consistency.\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\text{Purity } = \\frac{1}{N} \\sum_{i = 1}^k \\max_j n_{i,j}\n",
    "$$\n",
    "where:\n",
    "- $N = $ total number of points,\n",
    "- $k = $ number of clusters,\n",
    "- $n_{i,j} = $​ number of points from class $j$ in cluster $i$,\n",
    "- $\\max_j n_{i,j} = $ number of points from the most common class in cluster $i$.\n",
    "\n",
    "\n",
    "### Entropy\n",
    "\n",
    "Entropy measures how mixed the classes are within a cluster. Low entropy means most points in a cluster belong to the same class. High entropy means points are more evenly distributed across different classes.\n",
    "\n",
    "**Example:** Consider a fruit basket that is mostly filled with red apples, with only a few bananas and oranges. Since the basket is dominated by one type of fruit, it has low entropy. In contrast, if the basket contains an equal mix of apples, bananas, and oranges, the distribution is more random, resulting in high entropy. This even distribution means it is harder to predict the dominant fruit just by looking at the basket.\n",
    "\n",
    "***Formula for a single cluster:***\n",
    "$$\n",
    "E_i = -\\sum_{j=1}^{C} p_{ij} \\log_2(p_{ij})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $C = $ number of classes,\n",
    "- $p_{i,j} = $ proportion of points from class jj in cluster ii.\n",
    "\n",
    "The overall entropy is the weighted average across all clusters:\n",
    "\n",
    "$$\n",
    "\\text{Entropy} = \\frac{1}{N} \\sum_{i=1}^{k} n_i \\cdot E_i\n",
    "$$\n",
    "\n",
    "Where $n_i$​ is the number of points in cluster $i$.\n",
    "\n",
    "A good clustering aims for both high purity (most points in a cluster belong to one class) and low entropy (each cluster contains little class mixing).\n",
    "    \n",
    "<div class=\"alert alert-danger\">\n",
    " If this makes it easier for you to implement purity and entropy, you can modify the previously defined function `compute_metrics` to also return in the results dictionary the purity, the entropy or any other metric that you may want to use later on.\n",
    "</div>\n",
    "<div class=\"alert alert-danger\">\n",
    " Compared to the silhouette score which is computed using only the features, purity and entropy are metrics computed using the true label `y`. Do not forget to compute these metrics on a test set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3.2.2 : Purity and Entropy\n",
    "\n",
    "@pre:  A split of your dataset: X_train, X_test, y_train, y_test\n",
    "@post: A \"Purity/Entropy versus Number of Clusters\" plot. There should be two curves, one for the purity and one for the entropy.\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 3.2] Quality of the clustering </b>  <br>\n",
    "    You considered three different measures for the quality of the clustering: the first one is the silhouette score and is oblivious to the true labels: it is a truly unsupervised metric. The second and third metric use the true label to assess the quality of the clustering. Based on this observation,\n",
    "    \n",
    "1. Comment on the evolution of each metric according to the number of clusters.\n",
    "2. Comment on what do you now think is the ideal number of clusters ?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART 4 - Fairness metrics</b> </font> <br><br>\n",
    "\n",
    "Congratulations for reaching this far ! So far, you have thoroughly analyzed a sensitive dataset, you cleaned it and focused on what you believe were useful features for predicting recidivism. You then used the K-Means algorithm to have your own recidivism predictor.\n",
    "\n",
    "Because of the sensitivity of the dataset and its potential negative impact on certain parts of the population, you should now assess its fairness with respect to each gender and race group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>4.1 False Positive Rate</b> <br>\n",
    "</font>\n",
    "\n",
    "The false positive rate (FPR) is a performance metric used to evaluate the accuracy of a machine learning model, particularly in binary classification tasks. It refers to the proportion of actual negative instances (people that did not recidivate) that are incorrectly classified as positive. A lower FPR indicates that the model is better at identifying negative cases.\n",
    "\n",
    "A fair model would have the same FPR across all groups.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    " As for the purity and entropy metrics, the false positive rate metric uses the true labels, you should therefore make a train/test split before hand.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°4.1 False Positive Rate\n",
    "\n",
    "@pre:  A split of your dataset: X_train, X_test, y_train, y_test\n",
    "@post: A \"False Positive Rate vs Number of Clusters\" plot for each group\n",
    "\"\"\"\n",
    "\n",
    "# Because the dataset is imbalanced, we will repartition our dataset into three race groups: African-American, Caucasian and Other.\n",
    "group_labels = np.where(race == 0, 0, np.where(race == 1, 1, 2))\n",
    "\n",
    "# X_train, X_test, y_train, y_test, group_train, group_val = train_test_split(\n",
    "#     X, y, group_labels, test_size=0.2, random_state=random_seed\n",
    "# )\n",
    "\n",
    "# Doing so, you can now use X_test[group_val == i] to get the test points with race i.\n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"False Positive Rate\")\n",
    "plt.title(\"False Positive Rate vs. Number of Clusters\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>4.2 Demographic Parity</b> <br>\n",
    "</font>\n",
    "Demographic parity is a fairness metric aimed at ensuring that a machine learning model’s predictions do not depend on membership in a sensitive group. Specifically, demographic parity is achieved when the likelihood of a prediction is independent of sensitive group membership. In binary classification, demographic parity requires equal selection rates across groups.\n",
    "\n",
    "In our case, perfect demographic parity means that there is the exact same proportion of “bail denied” in each race group. A fair model would have the same Demographic Parity value across all groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°4.2 Demographic Parity\n",
    "\n",
    "@pre:  A split of your dataset: X_train, X_test, y_train, y_test\n",
    "@post: A \"Demographic Parity vs Number of Clusters\" plot\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# We provide the function below to compute demographic parity\n",
    "def compute_demographic_parity(y_pred, group_labels):\n",
    "    unique_groups = np.unique(group_labels)\n",
    "    demographic_parity = {}\n",
    "\n",
    "    for group in unique_groups:\n",
    "        # Create a boolean mask for the current group\n",
    "        group_mask = group_labels == group\n",
    "\n",
    "        # Calculate the proportion of positive predictions for the group\n",
    "        group_pred = y_pred[group_mask]\n",
    "        positive_rate = np.mean(group_pred == 1)\n",
    "\n",
    "        demographic_parity[group] = positive_rate\n",
    "\n",
    "    return demographic_parity\n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Demographic Parity\")\n",
    "plt.title(\"Demographic Parity vs. Number of Clusters\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 4.1] Fairness of your model </b>  <br>\n",
    "    You considered two different measures for the fairness of your model and checked for various variants of your algorithm (number of clusters) the value of these fairness metrics.\n",
    "\n",
    "Is your algorithm unfair ? If yes, which ethnic group is penalized by the unfairness of your model ?\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 4.2] Presence of the sensitive features in the dataset [BONUS]</b> <br> \n",
    "In Cell 1.5, you removed the sensitive features from your dataset before building your algorithm. Yet, you may have noticed unfairness in your algorithm.\n",
    "\n",
    "1. Provide reasons why it is not necessarily enough to remove sensitive features from your dataset if you want to have fair predictions.\n",
    "2. Compute FPR and Demographic Parity for your algorithm when trained on the full dataset. Is the fairness of your classifier worse ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for the BONUS question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART 5 - Visualization </b> </font> <br><br>\n",
    "\n",
    "<font size=5 color=#009999> <b>5.1 Visualize your results</b> <br>\n",
    "</font>\n",
    "In the last cell, you can create the figure of your choice to visualize your results. You can be as creative as you want as long as you only use one figure (with potentially more than one plot).\n",
    "\n",
    "You will be evaluated on the clarity of your figure. You should ask yourself the following question while creating it: \"Is the message I am trying to convey clear enough so that a student from another group can take a quick look and understand it directly ?\" If the answer is positive, it's probably a great plot !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for the VISUALIZATION question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
